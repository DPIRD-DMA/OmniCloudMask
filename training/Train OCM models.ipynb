{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Dataset Information\n",
    "\n",
    "**Make sure all the the notebooks in the \"Get datasets\" folder have completed before running this**\n",
    "\n",
    "This Notebook will train on the following datasets\n",
    "\n",
    "- CloudSEN12 high (train (L1C+L2A))\n",
    "- CloudSEN12 scribble (train + val + test (L1C+L2A))\n",
    "- CloudSEN12 2k (train + val + test (L1C+L2A))\n",
    "- Kappaset (train + val + test (L1C))\n",
    "- CloudSEN12 high (train (L1C)) super res 5m\n",
    "- CloudSEN12 high from Planetary Computer (L2A)\n",
    "- A custom hard negative dataset (L2A)\n",
    "\n",
    "It validates on the CloudSEN12 high validation (L1C+L2A) dataset.\n",
    "\n",
    "Each model takes about 6 hours to train on a 4090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import rasterio as rio\n",
    "from fastai.vision.all import * # type: ignore\n",
    "from pathlib import Path\n",
    "from safetensors.torch import save_file\n",
    "import timm\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.errors import NotGeoreferencedWarning\n",
    "import warnings\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=NotGeoreferencedWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from augs import (\n",
    "    BatchRot90,\n",
    "    RandomRectangle,\n",
    "    DynamicZScoreNormalize,\n",
    "    SceneEdge,\n",
    "    BatchTear,\n",
    "    BatchResample,\n",
    "    RandomClipLargeImages,\n",
    "    RandomSharpenBlur,\n",
    "    ClipHighAndLow,\n",
    "    BatchFlip,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    DiceMultiStrip,\n",
    "    CrossEntropyLossFlatImageTypeWeighted,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import plot_batch, show_histo, print_system_info\n",
    "\n",
    "print_system_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset_dir = Path(\"/media/nick/4TB Working 7/Datasets/OCM datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudsen12_high_data_dir = base_dataset_dir / \"CloudSEN12 high\"\n",
    "cloudsen12_scribble_dir = base_dataset_dir / \"CloudSEN12 scribble\"\n",
    "cloudsen12_k2_dir = base_dataset_dir / \"CloudSEN12 2k\"\n",
    "cloudsen12_validation_dir = base_dataset_dir / \"CloudSEN12 validation\"\n",
    "cloudsen12_high_planetary_computer_dir = (\n",
    "    base_dataset_dir / \"CloudSEN12 high planetary computer\"\n",
    ")\n",
    "super_res_dir = base_dataset_dir / \"CloudSEN12 high super res tiles\"\n",
    "\n",
    "kappaset_data_dir = base_dataset_dir / \"Kappaset\"\n",
    "hard_negative_data_dir = base_dataset_dir / \"Hard negative\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_type = \"regnety_004.pycls_in1k\"\n",
    "model_type = \"edgenext_small.usi_in1k\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = \"OCM_7.43_R_G_NIR_test\"\n",
    "#########################\n",
    "use_bf16 = True\n",
    "#########################\n",
    "demo_mode = False\n",
    "#########################\n",
    "original_image_size = 509\n",
    "max_clip_image_clip_size = 400  # 509\n",
    "min_clip_image_size = 256  # 509\n",
    "limited_band_read_list = [1, 2, 3]  # Red Green NIR\n",
    "native_band_scales = [1, 1, 0.5]\n",
    "#########################\n",
    "gradient_accumulation_batch_size = 128\n",
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "#########################\n",
    "high_label_weight = 0.9\n",
    "scribble_label_weight = 0.5\n",
    "tiles_2k_label_weight = 0.5\n",
    "kappaset_label_weight = 0.25\n",
    "super_res_label_weight = 0.25\n",
    "high_pc_label_weight = 0.9\n",
    "hard_negative_label_weight = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_weights = {\n",
    "    cloudsen12_high_data_dir: high_label_weight,\n",
    "    cloudsen12_scribble_dir: scribble_label_weight,\n",
    "    cloudsen12_k2_dir: tiles_2k_label_weight,\n",
    "    super_res_dir: super_res_label_weight,\n",
    "    cloudsen12_high_planetary_computer_dir: high_pc_label_weight,\n",
    "    kappaset_data_dir: kappaset_label_weight,\n",
    "    hard_negative_data_dir: hard_negative_label_weight,\n",
    "    cloudsen12_validation_dir: 1.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dirs = label_weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_dir in dataset_dirs:\n",
    "    assert dataset_dir.exists(), (\n",
    "        f\"Training data directory {dataset_dir} does not exist.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if demo_mode:\n",
    "    freeze_epochs = 5\n",
    "    unfrozen_epochs = 5\n",
    "    limit_training_images = 3000\n",
    "else:\n",
    "    freeze_epochs = 15\n",
    "    unfrozen_epochs = 15\n",
    "    limit_training_images = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input_channels = len(limited_band_read_list)\n",
    "print(f\"Number of input channels: {num_input_channels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_model = partial(\n",
    "    timm.create_model,\n",
    "    model_type,\n",
    "    pretrained=True,\n",
    "    in_chans=num_input_channels,\n",
    ")\n",
    "model = create_unet_model(\n",
    "    img_size=(509, 509),\n",
    "    arch=timm_model,\n",
    "    n_out=4,\n",
    "    pretrained=True,\n",
    "    act_cls=torch.nn.Mish,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(\n",
    "    1, num_input_channels, original_image_size, original_image_size\n",
    ")\n",
    "assert model(dummy_input).shape == (\n",
    "    1,\n",
    "    4,\n",
    "    original_image_size,\n",
    "    original_image_size,\n",
    "), \"Model output shape mismatch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fai_model_name = f\"PM_model_{model_version}_{model_type}_fai\"\n",
    "pytorch_model_name = f\"PM_model_{model_version}_{model_type}_PT.pth\"\n",
    "pytorch_model_path = Path.cwd() / \"models\" / pytorch_model_name\n",
    "state_path = pytorch_model_path.parent / f\"{pytorch_model_path.stem}_state.pth\"\n",
    "safetensor_state_path = (\n",
    "    pytorch_model_path.parent / f\"{pytorch_model_path.stem}_state.safetensors\"\n",
    ")\n",
    "config_path = pytorch_model_path.parent / f\"{pytorch_model_path.stem}_config.json\"\n",
    "if pytorch_model_path.exists():\n",
    "    raise ValueError(\"Model already exists\", pytorch_model_name)\n",
    "if state_path.exists():\n",
    "    raise ValueError(\"State path already exists\")\n",
    "if safetensor_state_path.exists():\n",
    "    raise ValueError(\"Safetensor state path already exists\")\n",
    "if config_path.exists():\n",
    "    raise ValueError(\"Config path already exists\")\n",
    "\n",
    "print(f\"Fastai model {fai_model_name}\")\n",
    "print(f\"PyTorch model {pytorch_model_name}\")\n",
    "print(f\"State path: {state_path}\")\n",
    "print(f\"Safetensor state path: {safetensor_state_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_dataset_getter(paths: list[Path], print_counts: bool = False):\n",
    "    training_images = []\n",
    "    validation_images = []\n",
    "    for path in paths:\n",
    "        if path == cloudsen12_validation_dir:\n",
    "            validation_images = list(path.glob(\"*image*.tif\"))\n",
    "            if print_counts:\n",
    "                print(f\"{path.name} found {len(validation_images)} validation images\")\n",
    "        else:\n",
    "            images = list(path.glob(\"*image*.tif\"))\n",
    "            if print_counts:\n",
    "                print(f\"{path.name} found {len(images)} images\")\n",
    "            training_images.extend(images)\n",
    "    if print_counts:\n",
    "        print(f\"Found {len(training_images)} training images\")\n",
    "\n",
    "    if limit_training_images:\n",
    "        # shuffle and limit training images\n",
    "        training_images = np.random.choice(\n",
    "            training_images, limit_training_images, replace=False\n",
    "        ).tolist()\n",
    "        if print_counts:\n",
    "            print(f\"Limited training images to {len(training_images)}\")\n",
    "\n",
    "    datasets = training_images + validation_images\n",
    "    if print_counts:\n",
    "        print(f\"Combined training and validation {len(datasets)} images\")\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_val_images = multi_dataset_getter(list(dataset_dirs), print_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = set(cloudsen12_validation_dir.glob(\"*image*.tif\"))\n",
    "len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_func(file_path):\n",
    "    file_name = file_path.name\n",
    "\n",
    "    label_name = (\n",
    "        file_name.replace(\"image\", \"label\").replace(\"_l1c\", \"\").replace(\"_l2a\", \"\")\n",
    "    )\n",
    "    label_path = file_path.parent / label_name\n",
    "\n",
    "    assert label_path.exists(), f\"Label path does not exist: {label_path}\"\n",
    "    assert file_path != label_path, (\n",
    "        f\"File path and label path are the same: {file_path}\"\n",
    "    )\n",
    "\n",
    "    return label_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_val_images[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_2k(src: rio.DatasetReader, img_size: int) -> np.ndarray:\n",
    "    # The 2k images are 2000, 2000 pixels at 10m resolution\n",
    "    # We resample them to img_size using a random resampling method\n",
    "    resampling_method = random.choice([Resampling.bilinear, Resampling.nearest])\n",
    "    resampled_data = src.read(\n",
    "        limited_band_read_list,\n",
    "        out_shape=(len(limited_band_read_list), img_size, img_size),\n",
    "        resampling=resampling_method,\n",
    "    )\n",
    "    return resampled_data.astype(\"float32\")\n",
    "\n",
    "\n",
    "scale_groups = defaultdict(list)\n",
    "for i, (band, scale) in enumerate(\n",
    "    zip(limited_band_read_list, native_band_scales, strict=True)\n",
    "):\n",
    "    scale_groups[int(original_image_size * scale)].append((i, band))\n",
    "\n",
    "\n",
    "def open_509(src: rio.DatasetReader, img_size: int) -> np.ndarray:\n",
    "    resampled_data = np.empty(\n",
    "        (len(limited_band_read_list), img_size, img_size), dtype=np.float32\n",
    "    )\n",
    "    resampling_method = random.choice([cv2.INTER_NEAREST, cv2.INTER_LINEAR])\n",
    "\n",
    "    for true_img_size, band_info in scale_groups.items():\n",
    "        indices, bands = zip(*band_info, strict=True)\n",
    "\n",
    "        if true_img_size == img_size:\n",
    "            native_bands = src.read(\n",
    "                bands,\n",
    "                out_shape=(len(bands), img_size, img_size),\n",
    "            )\n",
    "            resampled_data[np.array(indices)] = native_bands.astype(np.float32)\n",
    "        else:\n",
    "            native_bands = src.read(\n",
    "                bands,\n",
    "                out_shape=(len(bands), true_img_size, true_img_size),\n",
    "                resampling=Resampling.nearest,\n",
    "            )\n",
    "\n",
    "            for i, idx in enumerate(indices):\n",
    "                resized_int16 = cv2.resize(\n",
    "                    native_bands[i],\n",
    "                    (img_size, img_size),\n",
    "                    interpolation=resampling_method,\n",
    "                )\n",
    "                resampled_data[idx] = resized_int16.astype(np.float32)\n",
    "\n",
    "    return resampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_img(\n",
    "    img_path: Path,\n",
    "    img_size: int,\n",
    "    use_bf16: bool = False,\n",
    ") -> TensorImage:\n",
    "    with rio.open(img_path) as src:\n",
    "        profile = src.profile\n",
    "        # the 2k images we resample to img_size using a random resampling method\n",
    "        if profile[\"width\"] == 2000:\n",
    "            resampled_data = open_2k(src, img_size)\n",
    "        elif profile[\"width\"] == 509:\n",
    "            resampled_data = open_509(src, img_size)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported image width: {profile['width']}. Expected 2000 or 509.\"\n",
    "            )\n",
    "\n",
    "    image_tensor = torch.from_numpy(resampled_data)\n",
    "\n",
    "    if use_bf16:\n",
    "        image_tensor = image_tensor.bfloat16()\n",
    "\n",
    "    return TensorImage(image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_weights(image_path: Path) -> torch.Tensor:\n",
    "    try:\n",
    "        weight = torch.tensor(label_weights[image_path.parent], dtype=torch.float32)\n",
    "    except Exception as e:\n",
    "        raise ValueError(\n",
    "            f\"Image path {image_path} not found in label_weights dictionary.\"\n",
    "        ) from e\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tfms = [\n",
    "    RandomRectangle(  # Blocks out random rectangles in the image\n",
    "        p=0.6,\n",
    "        sl=0.1,\n",
    "        sh=0.5,\n",
    "    ),\n",
    "    BatchTear(0.1),  # Simulates an image tear\n",
    "    SceneEdge(p=0.1),  # Adds a scene edge to the image\n",
    "    IntToFloatTensor(1, 1),\n",
    "    BatchRot90(),  # Rotates the image by 90 degrees\n",
    "    DynamicZScoreNormalize(),  # Normalizes the image using dynamic z-score normalization\n",
    "    BatchResample(\n",
    "        max_scale=1.111, min_scale=0.07, plateau_min=0.33, plateau_max=1.0\n",
    "    ),  # Resamples the image to a random scale\n",
    "    RandomClipLargeImages(  # Clips large images to a random size\n",
    "        max_size=max_clip_image_clip_size, min_size=min_clip_image_size\n",
    "    ),\n",
    "    BatchFlip(),  # Flips the image horizontally or vertically\n",
    "    RandomSharpenBlur(min_factor=0.5, max_factor=1.5),  # Sharpens or blurs the image\n",
    "    ClipHighAndLow(\n",
    "        p=0.1, max_pct=0.05\n",
    "    ),  # Simulates sensor saturation by clipping high and low values\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_image_func = partial(open_img, img_size=original_image_size, use_bf16=use_bf16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_validation_item(item: Path):\n",
    "    return item in validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dblock = DataBlock(\n",
    "    blocks=[\n",
    "        TransformBlock([open_image_func]),\n",
    "        MaskBlock(codes=[0, 1, 2, 3]),\n",
    "        TransformBlock([sample_weights]),\n",
    "    ],\n",
    "    n_inp=1,\n",
    "    get_items=multi_dataset_getter,\n",
    "    get_y=[label_func, lambda x: x],\n",
    "    splitter=FuncSplitter(is_validation_item),\n",
    "    batch_tfms=batch_tfms,\n",
    "    item_tfms=[\n",
    "        Resize(original_image_size, method=\"squish\")\n",
    "    ],  # required to resize the 2 masks to 509\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = dblock.dataloaders(\n",
    "    size=original_image_size,\n",
    "    source=dataset_dirs,\n",
    "    bs=batch_size,\n",
    "    num_workers=6,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.train.dataset.tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.train.after_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.train.after_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = dl.one_batch()\n",
    "print(f\"Input shape: {batch[0].shape}\")\n",
    "print(f\"Label shape: {batch[1].shape}\")\n",
    "print(f\"Input mean: {batch[0].mean()}\")\n",
    "print(f\"Input std: {batch[0].std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batch = dl.valid.one_batch()\n",
    "print(f\"Input shape: {val_batch[0].shape}\")\n",
    "print(f\"Label shape: {val_batch[1].shape}\")\n",
    "print(f\"Input mean: {batch[0].mean()}\")\n",
    "print(f\"Input std: {batch[0].std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = dl.one_batch()\n",
    "\n",
    "band_labels = [\"B04\", \"B03\", \"B8A\"]\n",
    "plot_batch(batch[:2], labels=[\"False colour\"] + band_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = dl.one_batch()\n",
    "batch[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0][0][0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_histo(batch[:2], labels=band_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ShowGraphCallback(),\n",
    "    GradientAccumulation(gradient_accumulation_batch_size),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Learner(\n",
    "    dls=dl,\n",
    "    model=model,\n",
    "    loss_func=CrossEntropyLossFlatImageTypeWeighted(),\n",
    "    metrics=[DiceMultiStrip],\n",
    "    cbs=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_bf16:\n",
    "    learner = learner.to_bf16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fine_tune(\n",
    "    epochs=unfrozen_epochs,\n",
    "    freeze_epochs=freeze_epochs,\n",
    "    base_lr=learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save(fai_model_name)\n",
    "learner.load(fai_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learner.model.to(\"cpu\")\n",
    "model = model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, pytorch_model_path)\n",
    "pytorch_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), state_path)\n",
    "state_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file(model.state_dict(), safetensor_state_path)\n",
    "safetensor_state_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model_version\": model_version,\n",
    "    \"model_type\": model_type,\n",
    "    \"use_bf16\": use_bf16,\n",
    "    \"demo_mode\": demo_mode,\n",
    "    \"original_image_size\": original_image_size,\n",
    "    \"max_clip_image_clip_size\": max_clip_image_clip_size,\n",
    "    \"min_clip_image_size\": min_clip_image_size,\n",
    "    \"limited_band_read_list\": limited_band_read_list,\n",
    "    \"native_band_scales\": native_band_scales,\n",
    "    \"gradient_accumulation_batch_size\": gradient_accumulation_batch_size,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"high_label_weight\": high_label_weight,\n",
    "    \"scribble_label_weight\": scribble_label_weight,\n",
    "    \"tiles_2k_weight\": tiles_2k_label_weight,\n",
    "    \"kappaset_weight\": kappaset_label_weight,\n",
    "    \"super_res_weight\": super_res_label_weight,\n",
    "    \"high_pc_label_weight\": high_pc_label_weight,\n",
    "    \"hard_negative_label_weight\": hard_negative_label_weight,\n",
    "    \"freeze_epochs\": freeze_epochs,\n",
    "    \"unfrozen_epochs\": unfrozen_epochs,\n",
    "    \"limit_training_images\": limit_training_images,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_path, \"w\") as f:\n",
    "    json.dump(config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnicloudmask (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
