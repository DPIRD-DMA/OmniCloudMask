{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import rasterio as rio\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from fastai.vision.all import *\n",
    "from fastai.vision.learner import create_unet_model\n",
    "from pathlib import Path\n",
    "from safetensors.torch import save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    BatchRot90,\n",
    "    RandomRectangle,\n",
    "    DynamicZScoreNormalize,\n",
    "    SceneEdge,\n",
    "    BatchTear,\n",
    "    BatchResample,\n",
    "    RandomClipLargeImages,\n",
    "    RandomSharpenBlur,\n",
    "    ClipHighAndLow,\n",
    "    BatchFlip,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import plot_batch, show_histo, print_system_info\n",
    "\n",
    "print_system_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_dir = Path(\"/media/nick/4TB Working 7/Datasets/CloudSEN12\")\n",
    "image_cache_dir = Path(\"/media/nick/4TB Working 7/Datasets/CloudSEN12 training cache\")\n",
    "\n",
    "image_cache_dir.mkdir(exist_ok=True)\n",
    "\n",
    "assert (\n",
    "    training_data_dir.exists()\n",
    "), f\"Training data directory {training_data_dir} does not exist.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = \"OCM_6.45_RG_NIR\"\n",
    "bf16 = True\n",
    "demo_mode = False\n",
    "original_image_size = 509\n",
    "max_clip_image_clip_size = 400  # 509\n",
    "min_clip_image_size = 256  # 509\n",
    "limited_band_read_list = [1, 2, 3]  # Red Green NIR\n",
    "gradient_accumulation_batch_size = 128\n",
    "batch_size = 10\n",
    "cache_entire_dataset = True  # RAM hungry but will reduce training time\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_type = \"regnety_004.pycls_in1k\"\n",
    "# model_type = \"convnextv2_nano.fcmae_ft_in1k\"\n",
    "model_type = \"edgenext_small.usi_in1k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input_channels = len(limited_band_read_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_model = partial(\n",
    "    timm.create_model,\n",
    "    model_type,\n",
    "    pretrained=True,\n",
    "    in_chans=num_input_channels,\n",
    ")\n",
    "model = create_unet_model(\n",
    "    img_size=(509, 509),\n",
    "    arch=timm_model,\n",
    "    n_out=4,\n",
    "    pretrained=True,\n",
    "    act_cls=torch.nn.Mish,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(\n",
    "    1, num_input_channels, original_image_size, original_image_size\n",
    ")\n",
    "assert model(dummy_input).shape == (\n",
    "    1,\n",
    "    4,\n",
    "    original_image_size,\n",
    "    original_image_size,\n",
    "), \"Model output shape mismatch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fai_model_name = f\"PM_model_{model_version}_{model_type}_fai\"\n",
    "pytorch_model_name = f\"PM_model_{model_version}_{model_type}_PT.pth\"\n",
    "pytorch_model_path = Path.cwd() / \"models\" / pytorch_model_name\n",
    "state_path = pytorch_model_path.parent / f\"{pytorch_model_path.stem}_state.pth\"\n",
    "safetensor_state_path = (\n",
    "    pytorch_model_path.parent / f\"{pytorch_model_path.stem}_state.safetensors\"\n",
    ")\n",
    "if pytorch_model_path.exists():\n",
    "    raise ValueError(\"Model already exists\")\n",
    "if state_path.exists():\n",
    "    raise ValueError(\"State path already exists\")\n",
    "if safetensor_state_path.exists():\n",
    "    raise ValueError(\"Safetensor state path already exists\")\n",
    "print(f\"Fastai model {fai_model_name}\")\n",
    "print(f\"PyTorch model {pytorch_model_name}\")\n",
    "print(f\"State path: {state_path}\")\n",
    "print(f\"Safetensor state path: {safetensor_state_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if demo_mode:\n",
    "    freeze_epochs = 5\n",
    "    unfrozen_epochs = 5\n",
    "    limit_training_images = 3000\n",
    "    data_types = [\"l1c\"]\n",
    "else:\n",
    "    freeze_epochs = 15\n",
    "    unfrozen_epochs = 15\n",
    "    limit_training_images = 0\n",
    "    data_types = [\"l1c\", \"l2a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_files_custom(source):\n",
    "    all_images = []\n",
    "    for data_type in data_types:\n",
    "        all_images += list(source.glob(f\"*509_image_{data_type}.tif\"))\n",
    "    train_imgs = []\n",
    "    val_imgs = []\n",
    "    for image in all_images:\n",
    "        if \"train\" in image.name:\n",
    "            train_imgs.append(image)\n",
    "        if \"validation_509_image\" in image.name:\n",
    "            val_imgs.append(image)\n",
    "    print(f\"Training images: {len(train_imgs)}\")\n",
    "\n",
    "    if limit_training_images:\n",
    "        train_imgs.sort()\n",
    "        print(f\"Limiting training images to {limit_training_images}\")\n",
    "        train_imgs = train_imgs[:limit_training_images]\n",
    "\n",
    "    print(f\"Validation images: {len(val_imgs)}\")\n",
    "    train_and_val = train_imgs + val_imgs\n",
    "    print(f\"Total images: {len(train_and_val)}\")\n",
    "\n",
    "    return train_and_val\n",
    "\n",
    "\n",
    "def label_func(file_path):\n",
    "    file_name = file_path.name\n",
    "\n",
    "    label_name = (\n",
    "        file_name.replace(\"image\", \"label\").replace(\"_l1c\", \"\").replace(\"_l2a\", \"\")\n",
    "    )\n",
    "    label_path = file_path.parent / label_name\n",
    "\n",
    "    assert label_path.exists(), f\"Label path does not exist: {label_path}\"\n",
    "    assert (\n",
    "        file_path != label_path\n",
    "    ), f\"File path and label path are the same: {file_path}\"\n",
    "\n",
    "    return label_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_val_images = get_image_files_custom(training_data_dir)\n",
    "total_image_count = len(train_and_val_images)\n",
    "total_image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_img(\n",
    "    img_path: Path, img_size: int, image_cache: dict | None = None, bf16: bool = False\n",
    ") -> TensorImage:\n",
    "    with rio.open(img_path) as src:\n",
    "        raw_bands = src.read(\n",
    "            limited_band_read_list,\n",
    "            out_shape=(img_size, img_size),\n",
    "        ).astype(\"float32\")\n",
    "\n",
    "    image_tensor = torch.from_numpy(raw_bands)\n",
    "\n",
    "    if bf16:\n",
    "        image_tensor = image_tensor.bfloat16()\n",
    "    if image_cache is not None:\n",
    "        image_cache[img_path] = TensorImage(image_tensor)\n",
    "\n",
    "    return TensorImage(image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_and_val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cache_entire_dataset:\n",
    "    image_cache_dir_with_img_count = image_cache_dir / str(total_image_count)\n",
    "    print(f\"Image cache dir: {image_cache_dir_with_img_count}\")\n",
    "\n",
    "    def load_cache(image_cache_dir: Path):\n",
    "        image_cache = {}\n",
    "        for cache_file in progress_bar(\n",
    "            list(image_cache_dir.glob(\"*.pkl\")), comment=\"Loading cache\"  # type: ignore\n",
    "        ):\n",
    "            with open(cache_file, \"rb\") as f:\n",
    "                temp_dict = pickle.load(f)\n",
    "                for key, value in temp_dict.items():\n",
    "                    image_cache[key] = value\n",
    "        return image_cache\n",
    "\n",
    "    def chunks(l, n):\n",
    "        \"\"\"Yield n number of striped chunks from l.\"\"\"\n",
    "        for i in range(0, n):\n",
    "            yield l[i::n]\n",
    "\n",
    "    if image_cache_dir_with_img_count.exists():\n",
    "        print(\"Image Cache found, loading cache\")\n",
    "        image_cache = load_cache(image_cache_dir_with_img_count)\n",
    "\n",
    "    else:\n",
    "        print(\"Image Cache not found, creating cache\")\n",
    "        image_cache_dir_with_img_count.mkdir(exist_ok=True)\n",
    "        train_and_val_images_parts = list(chunks(train_and_val_images, 8))\n",
    "\n",
    "        for i, chunk in progress_bar(\n",
    "            enumerate(train_and_val_images_parts),\n",
    "            comment=\"Making cache\",  # type: ignore\n",
    "            total=len(train_and_val_images_parts),\n",
    "        ):\n",
    "            image_cache = {}\n",
    "            open_img_partial = partial(\n",
    "                open_img,\n",
    "                img_size=original_image_size,\n",
    "                image_cache=image_cache,\n",
    "                bf16=bf16,\n",
    "            )\n",
    "            with ThreadPool(6) as p:\n",
    "                list(\n",
    "                    progress_bar(\n",
    "                        p.imap(\n",
    "                            open_img_partial,\n",
    "                            chunk,\n",
    "                        ),\n",
    "                        total=len(chunk),\n",
    "                        leave=False,\n",
    "                        comment=\"Opening a chunk of images\",  # type: ignore\n",
    "                    )\n",
    "                )\n",
    "            with open(\n",
    "                image_cache_dir_with_img_count / f\"image_cache_{i}.pkl\", \"wb\"\n",
    "            ) as f:\n",
    "                pickle.dump(image_cache, f)\n",
    "        image_cache = {}\n",
    "\n",
    "        image_cache = load_cache(image_cache_dir_with_img_count)\n",
    "\n",
    "    def open_image_func(img_path: Path) -> TensorImage:\n",
    "        return image_cache[img_path]\n",
    "\n",
    "else:\n",
    "    open_image_func = partial(\n",
    "        open_img, img_size=original_image_size, image_cache=None, bf16=bf16\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tfms = [\n",
    "    RandomRectangle(  # Blocks out random rectangles in the image\n",
    "        p=0.6,\n",
    "        sl=0.1,\n",
    "        sh=0.5,\n",
    "    ),\n",
    "    BatchTear(0.1),  # Simulates an image tear\n",
    "    SceneEdge(p=0.1),  # Adds a scene edge to the image\n",
    "    IntToFloatTensor(1, 1),\n",
    "    BatchRot90(),  # Rotates the image by 90 degrees\n",
    "    DynamicZScoreNormalize(),  # Normalizes the image using dynamic z-score normalization\n",
    "    BatchResample(\n",
    "        max_scale=1.111, min_scale=0.07, plateau_min=0.33, plateau_max=1.0\n",
    "    ),  # Resamples the image to a random scale\n",
    "    RandomClipLargeImages(  # Clips large images to a random size\n",
    "        max_size=max_clip_image_clip_size, min_size=min_clip_image_size\n",
    "    ),\n",
    "    BatchFlip(),  # Flips the image horizontally or vertically\n",
    "    RandomSharpenBlur(min_factor=0.5, max_factor=1.5),  # Sharpens or blurs the image\n",
    "    ClipHighAndLow(\n",
    "        p=0.1, max_pct=0.05\n",
    "    ),  # Simulates sensor saturation by clipping high and low values\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dblock = DataBlock(\n",
    "    blocks=[\n",
    "        TransformBlock([open_image_func]),\n",
    "        MaskBlock(codes=[0, 1, 2, 3]),\n",
    "    ],\n",
    "    get_items=get_image_files_custom,\n",
    "    get_y=label_func,\n",
    "    splitter=FuncSplitter(lambda o: \"validation\" in o.name),\n",
    "    batch_tfms=batch_tfms,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = dblock.dataloaders(\n",
    "    size=original_image_size,\n",
    "    source=training_data_dir,\n",
    "    bs=batch_size,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.train.dataset.tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.train.after_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.train.after_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = dl.one_batch()\n",
    "print(f\"Input shape: {batch[0].shape}\")\n",
    "print(f\"Label shape: {batch[1].shape}\")\n",
    "print(f\"Input mean: {batch[0].mean()}\")\n",
    "print(f\"Input std: {batch[0].std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batch = dl.valid.one_batch()\n",
    "print(f\"Input shape: {val_batch[0].shape}\")\n",
    "print(f\"Label shape: {val_batch[1].shape}\")\n",
    "print(f\"Input mean: {batch[0].mean()}\")\n",
    "print(f\"Input std: {batch[0].std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = dl.one_batch()\n",
    "\n",
    "band_labels = [\"B04\", \"B03\", \"B8A\"]\n",
    "plot_batch(batch, labels=[\"False colour\"] + band_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0][0][0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_histo(batch, labels=band_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ShowGraphCallback(),\n",
    "    GradientAccumulation(gradient_accumulation_batch_size),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Learner(\n",
    "    dls=dl,\n",
    "    model=model,\n",
    "    loss_func=CrossEntropyLossFlat(axis=1),\n",
    "    metrics=[DiceMulti],\n",
    "    cbs=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bf16:\n",
    "    print(\"Using BF16\")\n",
    "    learner = learner.to_bf16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fine_tune(\n",
    "    epochs=unfrozen_epochs,\n",
    "    freeze_epochs=freeze_epochs,\n",
    "    base_lr=learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fai_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save(fai_model_name)\n",
    "\n",
    "learner.load(fai_model_name)\n",
    "learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learner.model.to(\"cpu\")\n",
    "model = model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, pytorch_model_path)\n",
    "pytorch_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), state_path)\n",
    "state_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file(model.state_dict(), safetensor_state_path)\n",
    "safetensor_state_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnicloudmask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
