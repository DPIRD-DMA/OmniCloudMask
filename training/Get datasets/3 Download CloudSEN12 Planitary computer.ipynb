{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c87f0e34",
   "metadata": {},
   "source": [
    "This Notebook will download the CloudSEN12 \"high\" labeled images from Planetary Computer, make sure you have already downloaded the CloudSEN12 data from Hugging Face first, using the other CloudSEN12 notebook.\n",
    "\n",
    "The only thing you will need to change in this is the base_dataset_dir to a local drive with 300 GB of available storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2914740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio as rio\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Tuple, Optional, List\n",
    "import rioxarray\n",
    "import pystac_client\n",
    "import planetary_computer as pc\n",
    "from shutil import copyfile\n",
    "from pystac.item import Item\n",
    "from pystac_client.stac_api_io import StacApiIO\n",
    "from urllib3 import Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset_dir = Path(\"/media/nick/4TB Working 7/Datasets/OCM datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d42d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_planetary_computer_dir = base_dataset_dir / \"CloudSEN12 high planetary computer\"\n",
    "high_planetary_computer_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e40c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [\"B04\", \"B03\", \"B8A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f48286",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_dir = base_dataset_dir / \"CloudSEN12 high\"\n",
    "if not high_dir.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"High directory {high_dir} does not exist.\"\n",
    "        f\" Please download the CloudSEN12 dataset from Hugging Face first.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a17926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.WARNING, format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "\n",
    "def parse_cloudsen12_filename(filename: str) -> Tuple[str, datetime, str]:\n",
    "    \"\"\"Parse CloudSEN12 filename to extract metadata.\"\"\"\n",
    "    pattern = r\"CloudSEN12_ROI_(\\d+)__(\\d{8}T\\d{6})_\\d{8}T\\d{6}_([A-Z0-9]+)_\"\n",
    "    match = re.search(pattern, filename)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Cannot parse filename: {filename}\")\n",
    "\n",
    "    roi_id, datetime_str, mgrs_tile = match.groups()\n",
    "    # Remove T prefix from MGRS tile if present\n",
    "    clean_mgrs_tile = mgrs_tile[1:] if mgrs_tile.startswith(\"T\") else mgrs_tile\n",
    "\n",
    "    # Parse datetime\n",
    "    date_obj = datetime.strptime(datetime_str, \"%Y%m%dT%H%M%S\")\n",
    "\n",
    "    return roi_id, date_obj, clean_mgrs_tile\n",
    "\n",
    "\n",
    "def find_sentinel2_item(mgrs_tile: str, target_date: datetime) -> Optional[Item]:\n",
    "    \"\"\"Find matching Sentinel-2 item using broad date search then metadata filtering.\"\"\"\n",
    "    retry = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[502, 503, 504],\n",
    "        allowed_methods=None,\n",
    "    )\n",
    "    stac_api_io = StacApiIO(max_retries=retry)\n",
    "\n",
    "    catalog = pystac_client.Client.open(\n",
    "        \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "        modifier=pc.sign_inplace,\n",
    "        stac_io=stac_api_io,\n",
    "    )\n",
    "\n",
    "    # Extract timestamp for datatake_id filtering\n",
    "    timestamp_str = target_date.strftime(\"%Y%m%dT%H%M%S\")\n",
    "\n",
    "    # Broad date search (Â±3 days)\n",
    "    start_date = (target_date - timedelta(days=3)).strftime(\"%Y-%m-%d\")\n",
    "    end_date = (target_date + timedelta(days=3)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    search = catalog.search(\n",
    "        collections=[\"sentinel-2-l2a\"],\n",
    "        datetime=f\"{start_date}/{end_date}\",\n",
    "        query={\"s2:mgrs_tile\": {\"eq\": mgrs_tile}},\n",
    "        limit=100,\n",
    "    )\n",
    "\n",
    "    items = list(search.items())\n",
    "\n",
    "    if not items:\n",
    "        logging.warning(\n",
    "            f\"No items found for MGRS {mgrs_tile} in \"\n",
    "            f\"date range {start_date} to {end_date}\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    # Filter by datatake_id containing our timestamp\n",
    "    matching_items = []\n",
    "\n",
    "    for item in items:\n",
    "        datatake_id = item.properties.get(\"s2:datatake_id\", \"\")\n",
    "        if timestamp_str in datatake_id:\n",
    "            matching_items.append(item)\n",
    "\n",
    "    if len(matching_items) == 0:\n",
    "        logging.info(\n",
    "            f\"No items found with timestamp {timestamp_str} in \"\n",
    "            f\"datatake_id for MGRS {mgrs_tile}\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    if len(matching_items) > 1:\n",
    "        logging.info(\n",
    "            f\"Found {len(matching_items)} processing versions for \"\n",
    "            f\"timestamp {timestamp_str}, using first\"\n",
    "        )\n",
    "    # Use the first match (handles multiple processing levels)\n",
    "    return matching_items[0]\n",
    "\n",
    "\n",
    "def download_pc_bands(\n",
    "    item,\n",
    "    output_path: Path,\n",
    "    reference_file: Path,\n",
    "    bands: List[str],\n",
    ") -> None:\n",
    "    \"\"\"Download specified bands from Planetary Computer and save as GeoTIFF.\"\"\"\n",
    "\n",
    "    # Get reference file properties\n",
    "    with rio.open(reference_file) as ref:\n",
    "        ref_crs = ref.crs\n",
    "        ref_transform = ref.transform\n",
    "        ref_bounds = ref.bounds\n",
    "        ref_shape = (ref.height, ref.width)\n",
    "\n",
    "    # Download each band\n",
    "    band_arrays = []\n",
    "\n",
    "    for band_name in bands:\n",
    "        if band_name not in item.assets:\n",
    "            raise ValueError(f\"Band {band_name} not available in item\")\n",
    "\n",
    "        # Get signed URL and open with rioxarray\n",
    "        signed_url = pc.sign(item.assets[band_name].href)\n",
    "\n",
    "        # Open and clip to reference bounds\n",
    "        band_da = rioxarray.open_rasterio(signed_url)\n",
    "\n",
    "        # Clip to reference file bounds and reproject to match reference\n",
    "        clipped = band_da.rio.clip_box(  # type: ignore\n",
    "            minx=ref_bounds.left,\n",
    "            miny=ref_bounds.bottom,\n",
    "            maxx=ref_bounds.right,\n",
    "            maxy=ref_bounds.top,\n",
    "            crs=ref_crs,\n",
    "        )\n",
    "\n",
    "        reprojected = clipped.rio.reproject(\n",
    "            ref_crs,\n",
    "            shape=ref_shape,\n",
    "            transform=ref_transform,\n",
    "        )\n",
    "\n",
    "        band_arrays.append(reprojected[0])\n",
    "\n",
    "    with rio.open(\n",
    "        output_path,\n",
    "        \"w\",\n",
    "        driver=\"GTiff\",\n",
    "        height=ref_shape[0],\n",
    "        width=ref_shape[1],\n",
    "        count=len(bands),\n",
    "        dtype=band_arrays[0].dtype,\n",
    "        crs=ref_crs,\n",
    "        transform=ref_transform,\n",
    "        compress=\"lzw\",\n",
    "    ) as dst:\n",
    "        for index, band in enumerate(band_arrays):\n",
    "            dst.write(band, index + 1)\n",
    "\n",
    "\n",
    "def process_file(\n",
    "    input_file: Path, output_dir: Path = high_planetary_computer_dir\n",
    ") -> bool:\n",
    "    \"\"\"Process a single CloudSEN12 file.\"\"\"\n",
    "    try:\n",
    "        _, target_date, mgrs_tile = parse_cloudsen12_filename(input_file.name)\n",
    "\n",
    "        output_filename = input_file.name.replace(\"_l2a.tif\", \"_l2a_PC.tif\")\n",
    "        output_path = output_dir / output_filename\n",
    "\n",
    "        if output_path.exists():\n",
    "            return True\n",
    "\n",
    "        # Find matching Sentinel-2 item\n",
    "        item = find_sentinel2_item(mgrs_tile, target_date)\n",
    "        if item is None:\n",
    "            logging.error(f\"No matching Sentinel-2 data found for {input_file.name}\")\n",
    "            return False\n",
    "\n",
    "        # Download and save\n",
    "        download_pc_bands(item, output_path, input_file, bands)\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {input_file.name}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36de16f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARNING, format=\"%(levelname)s: %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f76608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all L2A files\n",
    "l2a_files = list(high_dir.glob(\"*_l2a.tif\"))\n",
    "total_count = len(l2a_files)\n",
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f21505",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(32) as pool:\n",
    "    list(tqdm(pool.imap(process_file, l2a_files), total=total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e4de21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy over the labels from high to planetary computer\n",
    "high_labels = list(high_dir.glob(\"*label*.tif\"))\n",
    "len(high_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84481e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for high_label in tqdm(high_labels):\n",
    "    file_name = high_label.name\n",
    "    file_name = file_name.replace(\".tif\", \"_PC.tif\")\n",
    "    copyfile(high_label, high_planetary_computer_dir / file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnicloudmask (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
