{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36ee02e1",
   "metadata": {},
   "source": [
    "This notebook uses a super res model to convert the CloudSEN12 high dataset to 5m spatial res\n",
    "\n",
    "The only thing you will need to change in this is the base_dataset_dir to a local drive with 300 GB of available storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfebc4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from spandrel import ModelLoader\n",
    "from huggingface_hub import hf_hub_download\n",
    "import rasterio as rio\n",
    "from typing import Optional\n",
    "from rasterio.transform import Affine\n",
    "from multiprocessing import Pool\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e230bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset_dir = Path(\"/media/nick/4TB Working 7/Datasets/OCM datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4a3f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudsen12_high_dir = base_dataset_dir / \"CloudSEN12 high\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d03fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_res_raw_dir = base_dataset_dir / \"CloudSEN12 high super res raw\"\n",
    "super_res_raw_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9448de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_res_tile_dir = base_dataset_dir / \"CloudSEN12 high super res tiles\"\n",
    "super_res_tile_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b43246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs12_high_train_l1c_images = list(\n",
    "    cloudsen12_high_dir.glob(\"*train_509_high_image_l1c*.tif\")\n",
    ")\n",
    "len(cs12_high_train_l1c_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a9c453",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dtype = torch.bfloat16\n",
    "patch_size = 509\n",
    "overlap = 50\n",
    "device = torch.device(\"cuda\")\n",
    "upscale_factor = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bc4d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = hf_hub_download(\n",
    "    repo_id=\"Phips/2xNomosUni_esrgan_multijpg\",\n",
    "    filename=\"2xNomosUni_esrgan_multijpg.safetensors\",\n",
    ")\n",
    "\n",
    "model = ModelLoader().load_from_file(model_path)\n",
    "model.to(device).eval().to(inference_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fc5831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_stats(\n",
    "    input: torch.Tensor,\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Get the mean and std of a tensor across the last two dimensions.\"\"\"\n",
    "    channel_means = input.mean(dim=(-2, -1), keepdim=True)\n",
    "    channel_stds = input.std(dim=(-2, -1), keepdim=True)\n",
    "    max_val = input.max()\n",
    "    min_val = input.min()\n",
    "    return channel_means, channel_stds, max_val, min_val\n",
    "\n",
    "\n",
    "def transfer_colour_stats(\n",
    "    source: torch.Tensor,\n",
    "    target: Optional[torch.Tensor] = None,\n",
    "    target_std: Optional[torch.Tensor] = None,\n",
    "    target_mean: Optional[torch.Tensor] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Transfer mean and std from target to source\"\"\"\n",
    "    source_mean = source.mean(dim=(-2, -1), keepdim=True)\n",
    "    source_std = source.std(dim=(-2, -1), keepdim=True)\n",
    "\n",
    "    if target is not None:\n",
    "        target_mean = target.mean(dim=(-2, -1), keepdim=True)\n",
    "        target_std = target.std(dim=(-2, -1), keepdim=True)\n",
    "    elif target_std is None or target_mean is None:\n",
    "        raise ValueError(\"Either target or target_std and target_mean must be provided\")\n",
    "\n",
    "    normalised = (source - source_mean) / (source_std + 1e-8)\n",
    "    transferred = normalised * target_std + target_mean\n",
    "\n",
    "    return transferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12051a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in tqdm(cs12_high_train_l1c_images):\n",
    "    out_path = super_res_raw_dir / image_path.name.replace(\"image\", \"image_super_res\")\n",
    "    if out_path.exists():\n",
    "        continue\n",
    "    src = rio.open(image_path)\n",
    "    image_array = src.read()\n",
    "    profile = src.profile\n",
    "\n",
    "    image_tensor = torch.from_numpy(image_array).to(device).to(inference_dtype)\n",
    "\n",
    "    input_channel_mean, input_channel_std, max_scene_value, min_scene_value = (\n",
    "        get_tensor_stats(image_tensor)\n",
    "    )\n",
    "\n",
    "    image_tensor = (image_tensor - min_scene_value) / max_scene_value\n",
    "\n",
    "    pred_tensor = model(image_tensor.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "    pred_tensor = transfer_colour_stats(\n",
    "        source=pred_tensor,\n",
    "        target_std=input_channel_std,\n",
    "        target_mean=input_channel_mean,\n",
    "    )\n",
    "\n",
    "    pred_array = pred_tensor.float().numpy(force=True).astype(image_array.dtype)\n",
    "\n",
    "    output_transform = profile[\"transform\"] * Affine.scale(1 / upscale_factor)\n",
    "\n",
    "    output_profile = profile.copy()\n",
    "    output_profile.update(\n",
    "        {\n",
    "            \"dtype\": pred_array.dtype,\n",
    "            \"count\": pred_array.shape[0],\n",
    "            \"height\": pred_array.shape[1],\n",
    "            \"width\": pred_array.shape[2],\n",
    "            \"transform\": output_transform,\n",
    "        }\n",
    "    )\n",
    "    with rio.open(out_path, \"w\", **output_profile) as dst:\n",
    "        dst.write(pred_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f2b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_images = list(super_res_raw_dir.glob(\"*image_super_res*.tif\"))\n",
    "len(super_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01da03d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the top left of each image and mask and save it\n",
    "# for super_image in tqdm(super_images):\n",
    "def process_super_image(super_image):\n",
    "    src = rio.open(super_image)\n",
    "    profile = src.profile\n",
    "    array = src.read()\n",
    "    original_transform = profile[\"transform\"]\n",
    "\n",
    "    label_path = cloudsen12_high_dir / super_image.name.replace(\n",
    "        \"image_super_res\", \"label\"\n",
    "    ).replace(\"_l1c\", \"\")\n",
    "    label_crs = rio.open(label_path)\n",
    "    label_array = label_crs.read()\n",
    "    # use numpy to repeat the label on x andd y axes x2\n",
    "    label_array = label_array.repeat(upscale_factor, axis=1).repeat(\n",
    "        upscale_factor, axis=2\n",
    "    )\n",
    "\n",
    "    for col in [0, 1]:\n",
    "        for row in [0, 1]:\n",
    "            new_transform = original_transform * Affine.translation(\n",
    "                col * patch_size, row * patch_size\n",
    "            )\n",
    "            out_path = super_res_tile_dir / super_image.name.replace(\n",
    "                \"image_super_res\", f\"image_super_res_tile_{row}_{col}\"\n",
    "            )\n",
    "            if not out_path.exists():\n",
    "                array_clip = array[\n",
    "                    :,\n",
    "                    row * patch_size : row * patch_size + patch_size,\n",
    "                    col * patch_size : col * patch_size + patch_size,\n",
    "                ]\n",
    "                export_profile = profile.copy()\n",
    "                export_profile.update(\n",
    "                    {\n",
    "                        \"height\": patch_size,\n",
    "                        \"width\": patch_size,\n",
    "                        \"transform\": new_transform,\n",
    "                    }\n",
    "                )\n",
    "                with rio.open(out_path, \"w\", **export_profile) as dst:\n",
    "                    dst.write(array_clip)\n",
    "\n",
    "            label_out_path = super_res_tile_dir / label_path.name.replace(\n",
    "                \"label\", f\"label_super_res_tile_{row}_{col}\"\n",
    "            )\n",
    "            if not label_out_path.exists():\n",
    "                label_array_clip = label_array[\n",
    "                    :,\n",
    "                    row * patch_size : row * patch_size + patch_size,\n",
    "                    col * patch_size : col * patch_size + patch_size,\n",
    "                ]\n",
    "\n",
    "                label_profile = label_crs.profile.copy()\n",
    "\n",
    "                label_profile.update(\n",
    "                    {\n",
    "                        \"height\": patch_size,\n",
    "                        \"width\": patch_size,\n",
    "                        \"transform\": new_transform,\n",
    "                    }\n",
    "                )\n",
    "                with rio.open(label_out_path, \"w\", **label_profile) as dst:\n",
    "                    dst.write(label_array_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398e5b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(4) as pool:\n",
    "    list(\n",
    "        tqdm(\n",
    "            pool.imap(process_super_image, super_images),\n",
    "            total=len(super_images),\n",
    "            desc=\"Processing super images\",\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
